{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vào tối 22/10, sau đêm bán kết của cuộc thi Ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tân Hoa Xã đưa tin rằng vào ngày 9/10, phát ng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chuyến thăm của Lloyd Austin diễn ra chỉ vài n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trong dự thảo Luật Nhà giáo, Bộ GD&amp;ĐT đã đưa r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trong buổi gặp gỡ, Chủ tịch Hà Thị Nga đã bày ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43898</th>\n",
       "      <td>Giá vàng tiếp tục tăng phiên thứ hai, đạt đỉnh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43899</th>\n",
       "      <td>Để tỏ lòng tri ân với lịch sử và những người c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43900</th>\n",
       "      <td>MangoTV vừa công bố danh sách phim có lượt xem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43901</th>\n",
       "      <td>Để hoạt động đấu giá khoáng sản minh bạch và h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43902</th>\n",
       "      <td>Vào lúc 19 giờ, tâm áp thấp nhiệt đới nằm ở ph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43903 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content\n",
       "0      Vào tối 22/10, sau đêm bán kết của cuộc thi Ho...\n",
       "1      Tân Hoa Xã đưa tin rằng vào ngày 9/10, phát ng...\n",
       "2      Chuyến thăm của Lloyd Austin diễn ra chỉ vài n...\n",
       "3      Trong dự thảo Luật Nhà giáo, Bộ GD&ĐT đã đưa r...\n",
       "4      Trong buổi gặp gỡ, Chủ tịch Hà Thị Nga đã bày ...\n",
       "...                                                  ...\n",
       "43898  Giá vàng tiếp tục tăng phiên thứ hai, đạt đỉnh...\n",
       "43899  Để tỏ lòng tri ân với lịch sử và những người c...\n",
       "43900  MangoTV vừa công bố danh sách phim có lượt xem...\n",
       "43901  Để hoạt động đấu giá khoáng sản minh bạch và h...\n",
       "43902  Vào lúc 19 giờ, tâm áp thấp nhiệt đới nằm ở ph...\n",
       "\n",
       "[43903 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(DATA_PATH + \"articles_testing.tsv\", sep=\"\\t\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"./xlm-roberta-keywordtagger\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./xlm-roberta-keywordtagger\")\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device='cuda')\n",
    "\n",
    "example = df['content'][0]\n",
    "# Print len token\n",
    "print(len(tokenizer.tokenize(example)))\n",
    "\n",
    "\n",
    "def get_ner_results(text):\n",
    "    ner_results = nlp(text)\n",
    "    keywords = []\n",
    "    current_word = \"\"\n",
    "    for result in ner_results:\n",
    "        if result['entity'] == 'B':\n",
    "            if current_word != \"\":\n",
    "                keywords.append(current_word)\n",
    "                current_word = \"\"\n",
    "            current_word = result['word'].strip('▁')\n",
    "        if result['entity'] == 'I':\n",
    "            if result['word'][0] == '▁':\n",
    "                current_word += \" \" + result['word'].strip('▁')\n",
    "            else:\n",
    "                current_word += result['word']\n",
    "\n",
    "    # Add the last word\n",
    "    if current_word != \"\":\n",
    "        keywords.append(current_word)\n",
    "\n",
    "    return set(keywords)\n",
    "\n",
    "\n",
    "def process_long_text(text: str, tokenizer, max_length=512):\n",
    "    # Check if text needs splitting\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = tokens['input_ids'][0]\n",
    "\n",
    "    if len(input_ids) <= max_length:\n",
    "        return ','.join(get_ner_results(text))\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = text.split('.')\n",
    "    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n",
    "\n",
    "    # Combine sentences into chunks\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check length with new sentence\n",
    "        test_text = ' '.join(current_chunk + [sentence])\n",
    "        test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        test_length = len(test_tokens['input_ids'][0])\n",
    "        if test_length > max_length:\n",
    "            continue\n",
    "        if current_length + test_length <= max_length:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += test_length\n",
    "        else:\n",
    "            # Save current chunk and start new one\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = len(\n",
    "                tokenizer(sentence, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    # Process chunks and combine results\n",
    "    all_entities = set()\n",
    "    for chunk in chunks:\n",
    "        chunk_entities = get_ner_results(chunk)\n",
    "        all_entities.update(chunk_entities)\n",
    "\n",
    "    return ','.join(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# Process all articles\n",
    "df['tags'] = df['content'].apply(\n",
    "    lambda x: process_long_text(x, tokenizer))\n",
    "\n",
    "df.to_csv('./nhom1_sol2.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xal4food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
